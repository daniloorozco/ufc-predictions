{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ufc-predictions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPW8j9KNdhXZ"
      },
      "source": [
        "# **UFC _ Predictions**\r\n",
        "Machine Learning model to predict the winner.\r\n",
        "Make sure cell blocks are compiled and run sequentially"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go53NFRhfjkd"
      },
      "source": [
        "!pip install feature_engine\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDotvd6ifKu6"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from feature_engine import imputation as mdi\n",
        "from feature_engine import encoding as ce\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "#matplotlib inline\n",
        "with open('data.csv') as f:\n",
        "    df = pd.read_csv(f)\n",
        "f.close()\n",
        "df.info()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN_jRgRapHPV"
      },
      "source": [
        "with open('data.csv', newline='') as f:\r\n",
        "  reader = csv.reader(f)\r\n",
        "  row1 = next(reader)  # gets the first line\r\n",
        "  # now do something here\r\n",
        "  print(row1)\r\n",
        "  # if first row is the header, then you can do one more next() to get the next row:\r\n",
        "  # row2 = next(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSxUh__21fQp"
      },
      "source": [
        "#change winner (red/blue) to 1 or 0\r\n",
        "df['Winner'] = df['Winner'].replace(['Red'],'0')\r\n",
        "df['Winner'] = df['Winner'].replace(['Blue'],'1')\r\n",
        "df['Winner'] = df['Winner'].replace(['Draw'],'2')\r\n",
        "df['Winner'] = pd.to_numeric(df[\"Winner\"])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IdsmcsbkECW"
      },
      "source": [
        "#feature variability\n",
        "for col in df.columns:\n",
        "    print(col, df[col].nunique(), len(df))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZmO-dl1cXI0"
      },
      "source": [
        "Drop unecessary columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM85V-jMw2hv"
      },
      "source": [
        "df.drop(['R_fighter'], axis=1, inplace=True)\n",
        "df.drop(['B_fighter'], axis=1, inplace=True)\n",
        "df.drop(['Referee'], axis=1, inplace=True)\n",
        "df.drop(['location'], axis=1, inplace=True)\n",
        "df.drop(['date'], axis=1, inplace=True)\n",
        "df.drop(['title_bout'], axis=1, inplace=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ijo2cQMfEuog"
      },
      "source": [
        "for var in df.columns:\n",
        "    print(var, '\\n', df[var].value_counts()/len(df))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLcJRpqCYdNx"
      },
      "source": [
        "Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc7h_kcyYuQR"
      },
      "source": [
        "for var in df.columns:\n",
        "    if df[var].isnull().sum()/len(df) > 0:\n",
        "        print(var, df[var].isnull().mean().round(3))\n",
        "#df.drop('tenure_termed', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2RoeTXCcNaV"
      },
      "source": [
        "Train and Test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkaC8LOq-Rot"
      },
      "source": [
        "#training and test splitting\n",
        "y = df['Winner']\n",
        "X = df.drop('Winner', axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HeBw1rmopuX"
      },
      "source": [
        "Scatterplot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zgTj6o6on9A"
      },
      "source": [
        "sns.scatterplot(x='B_avg_CLINCH_att', y='R_avg_opp_CLINCH_landed', hue='Winner', data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH1TI38LcQhS"
      },
      "source": [
        "Imputations and data encoding on missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqoe21MiEcP9"
      },
      "source": [
        "# impute categorical features with more than 5% missing values w/ a new category 'missing'\n",
        "process_pipe = make_pipeline(\n",
        "    mdi.CategoricalImputer(variables=['R_Stance', 'B_Stance'\n",
        "                                              ], imputation_method='missing'),\n",
        "                             \n",
        "\n",
        "  mdi.ArbitraryNumberImputer(arbitrary_number = 30, variables=['R_age', 'B_age']),\n",
        "\n",
        "# Imputing missing values for numerical feature 'days_since_review' with an arbitrary digit\n",
        "    mdi.ArbitraryNumberImputer(arbitrary_number = 0.0, variables=['B_avg_BODY_att', 'B_avg_BODY_landed', 'B_avg_CLINCH_att', 'B_avg_CLINCH_landed', 'B_avg_DISTANCE_att', 'B_avg_DISTANCE_landed', 'B_Reach_cms', 'R_Reach_cms', 'B_avg_GROUND_att', 'B_avg_GROUND_landed', 'B_avg_HEAD_att', 'B_avg_HEAD_landed', 'B_avg_KD', 'B_avg_LEG_att', 'B_avg_LEG_landed', 'B_avg_PASS', 'B_avg_REV', 'B_avg_SIG_STR_att', 'B_avg_SIG_STR_landed', 'B_avg_SIG_STR_pct', 'B_avg_SUB_ATT', 'B_avg_TD_att', 'B_avg_TD_landed', 'B_avg_TD_pct', 'B_avg_TOTAL_STR_att', 'B_avg_TOTAL_STR_landed', 'B_avg_opp_BODY_att', 'B_avg_opp_BODY_landed', 'B_avg_opp_CLINCH_att', 'B_avg_opp_CLINCH_landed', 'B_avg_opp_DISTANCE_att', 'B_avg_opp_DISTANCE_landed', 'B_avg_opp_GROUND_att', 'B_avg_opp_GROUND_landed', 'B_avg_opp_HEAD_att', 'B_avg_opp_HEAD_landed', 'B_avg_opp_KD', 'B_avg_opp_LEG_att', 'B_avg_opp_LEG_landed', 'B_avg_opp_PASS', 'B_avg_opp_REV', 'B_avg_opp_SIG_STR_att', 'B_avg_opp_SIG_STR_landed', 'B_avg_opp_SIG_STR_pct', 'B_avg_opp_SUB_ATT', 'B_avg_opp_TD_att', 'B_avg_opp_TD_landed', 'B_avg_opp_TD_pct', 'B_avg_opp_TOTAL_STR_att', 'B_avg_opp_TOTAL_STR_landed',\n",
        "                                                                  'B_total_time_fought(seconds)', 'R_avg_BODY_att', 'R_avg_BODY_landed', 'R_avg_CLINCH_att', 'R_avg_CLINCH_landed', 'R_avg_DISTANCE_att', 'R_avg_DISTANCE_landed', 'R_avg_GROUND_att', 'R_avg_GROUND_landed', 'R_avg_HEAD_att', 'R_avg_HEAD_landed', 'R_avg_KD', 'R_avg_LEG_att', 'R_avg_LEG_landed', 'R_avg_PASS', 'R_avg_REV', 'R_avg_SIG_STR_att', 'R_avg_SIG_STR_landed', 'R_avg_SIG_STR_pct', 'R_avg_SUB_ATT', 'R_avg_TD_att', 'R_avg_TD_landed', 'R_avg_TD_pct', 'R_avg_TOTAL_STR_att', 'R_avg_TOTAL_STR_landed', \n",
        "                                                                  'R_avg_opp_BODY_att', 'R_avg_opp_BODY_landed', 'R_avg_opp_CLINCH_att', 'R_avg_opp_CLINCH_landed', 'R_avg_opp_DISTANCE_att', 'R_avg_opp_DISTANCE_landed', 'R_avg_opp_GROUND_att', 'R_avg_opp_GROUND_landed', 'R_avg_opp_HEAD_att', 'R_avg_opp_HEAD_landed', 'R_avg_opp_KD', 'R_avg_opp_LEG_att', 'R_avg_opp_LEG_landed', 'R_avg_opp_PASS', 'R_avg_opp_REV', 'R_avg_opp_SIG_STR_att', 'R_avg_opp_SIG_STR_landed', 'R_avg_opp_SIG_STR_pct', 'R_avg_opp_SUB_ATT', 'R_avg_opp_TD_att', 'R_avg_opp_TD_landed', 'R_avg_opp_TD_pct', 'R_avg_opp_TOTAL_STR_att', 'R_avg_opp_TOTAL_STR_landed', 'R_total_time_fought(seconds)'\n",
        "                                                                  ]),\n",
        "    \n",
        "# Target or Mean encoding for categorical features\n",
        "    ce.OrdinalEncoder(encoding_method='ordered',\n",
        "    variables=['weight_class', 'R_Stance', 'B_Stance'] \n",
        "))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbychVi4c0kg"
      },
      "source": [
        "process_pipe.fit(X_train, y_train)\r\n",
        "X_train_clean = process_pipe.transform(X_train)\r\n",
        "X_test_clean = process_pipe.transform(X_test)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "X_train_clean = X_train_clean.fillna(X_train_clean.mean())\r\n",
        "X_test_clean = X_test_clean.fillna(X_test_clean.mean())\r\n",
        "\r\n",
        "X_train_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0nNvdWYTynG"
      },
      "source": [
        "Machine Learning Training: **Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPBFUTT1T1lp"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "\r\n",
        "# all parameters not specified are set to their defaults\r\n",
        "model = LogisticRegression(solver='liblinear')\r\n",
        "model.fit(X_train_clean, y_train)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLLZ6h11jZ9E",
        "outputId": "4f9dfb32-9a5a-455d-8dc6-ea9e76248d7b"
      },
      "source": [
        "predictions = model.predict(X_test_clean)\r\n",
        "\r\n",
        "score = model.score(X_test_clean, y_test)\r\n",
        "score1 = model.score(X_train_clean, y_train)\r\n",
        "print(score)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6958211856171039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHzjeBz_eBJE"
      },
      "source": [
        "Model Performance Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsNncDdteIDX"
      },
      "source": [
        "cm = metrics.confusion_matrix(y_test, predictions)\r\n",
        "print(cm)\r\n",
        "\r\n",
        "plt.figure(figsize=(9,9))\r\n",
        "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\r\n",
        "plt.ylabel('Actual label');\r\n",
        "plt.xlabel('Predicted label');\r\n",
        "all_sample_title = 'Accuracy Score: {0}'.format(score)\r\n",
        "plt.title(all_sample_title, size = 15);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pumFfDqOU34i"
      },
      "source": [
        "Naive Bayes model training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K2hE2-v8jLu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0358bec-bfe9-405a-e505-b5f2ca7d5ea7"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.naive_bayes import BernoulliNB\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.naive_bayes import ComplementNB \r\n",
        "cnb = ComplementNB()\r\n",
        "bnb = BernoulliNB()\r\n",
        "mnb = MultinomialNB()\r\n",
        "gnb = GaussianNB()\r\n",
        "\r\n",
        "y_pred = cnb.fit(X_train_clean, y_train).predict(X_test_clean)\r\n",
        "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test_clean.shape[0], (y_test != y_pred).sum()))\r\n",
        "print(cnb.score(X_test_clean, y_test))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of mislabeled points out of a total 1029 points : 423\n",
            "0.5889212827988338\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6cJk8-OYShh"
      },
      "source": [
        "**SVM Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfrzB-2AYSuZ",
        "outputId": "54f8adea-1f31-490b-ecb9-b1bf767d55b2"
      },
      "source": [
        "from sklearn import svm\r\n",
        "modelSVM = svm.SVC()\r\n",
        "modelSVM.fit(X_train_clean, y_train)\r\n",
        "modelSVM.predict(X_test_clean)\r\n",
        "\r\n",
        "scoreSVM = modelSVM.score(X_test_clean, y_test)\r\n",
        "print(scoreSVM)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6919339164237124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X-clvTjeLA_"
      },
      "source": [
        "**Neural Networks**\r\n",
        "\r\n",
        "[Unbalanced resource api](https://imbalanced-learn.org/stable/user_guide.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXc5okAneN2T",
        "outputId": "f04834e3-ad37-49be-8e29-e0c98522fb3f"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\r\n",
        "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\r\n",
        "clf.fit(X_train_clean, y_train)\r\n",
        "\r\n",
        "y_pred1 = clf.predict(X_test_clean)\r\n",
        "scoreN = clf.score(X_test_clean, y_test)\r\n",
        "print(scoreN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6919339164237124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp2QZcy-uzTn"
      },
      "source": [
        "\r\n",
        "\r\n",
        "**Evaluations of Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymz2ZHsduuOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39ed80b8-5bfc-4949-96b5-b62ebae390bf"
      },
      "source": [
        "#evaluation of current model cross_validation\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.model_selection import cross_validate\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "\r\n",
        "lr_scores = cross_val_score(model, X_test_clean, y_test, cv=5)\r\n",
        "print(lr_scores)\r\n",
        "\r\n",
        "#validat method\r\n",
        "#It allows specifying multiple metrics for evaluation.\r\n",
        "#It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score.\r\n",
        "scoring = ['precision_macro', 'recall_macro']\r\n",
        "lr2_scores = cross_validate(model, X_test_clean, y_test, scoring=scoring)\r\n",
        "sorted(lr2_scores.keys())\r\n",
        "lr2_scores['test_recall_macro']"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.65048544 0.63106796 0.61650485 0.67475728 0.63414634]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.37872862, 0.37604204, 0.33982786, 0.3872457 , 0.35469881])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP-KqQmZ7Bpj"
      },
      "source": [
        "Testing kfold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD5s8xj47ANm"
      },
      "source": [
        "process_pipe.fit(X, y)\r\n",
        "X_clean = process_pipe.transform(X)\r\n",
        "X_clean.head()\r\n",
        "\r\n",
        "#kfold\r\n",
        "kf = KFold(n_splits=5)\r\n",
        "for train, test in kf.split(X_clean):\r\n",
        "  print(\"%s %s\" % (train, test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm4omlA5thRk"
      },
      "source": [
        "**Using the Model**\r\n",
        "*   Input fighters name to pull from database\r\n",
        "*   Model will predict who wins\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAlPUz-Utg4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b47008-f936-4f5e-e792-6cf0547fd43e"
      },
      "source": [
        "#check format\r\n",
        "with open('ufc259.csv') as f:\r\n",
        "    df1 = pd.read_csv(f)\r\n",
        "f.close()\r\n",
        "df1.info()\r\n",
        "df1.head()\r\n",
        "\r\n",
        "predictions_new = clf.predict(df1)\r\n",
        "\r\n",
        "print(predictions_new)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4 entries, 0 to 3\n",
            "Columns: 138 entries, weight_class to R_age\n",
            "dtypes: float64(102), int64(36)\n",
            "memory usage: 4.4 KB\n",
            "[0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}